# Self-MoA
We find that simply applying Mixture-of-Agents [1] on one single model (Self-MoA) could already improve the capability of LLM itself, resulting in new SOTAs on the popular AlpacaEval leaderboard! 

Stay tuned for more details!

[1] Wang, Junlin, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. “Mixture-of-Agents Enhances Large Language Model Capabilities.” arXiv, June 7, 2024. http://arxiv.org/abs/2406.04692.
