# Self-MoA
We find that simply applying Mixture-of-Agents [1] on one single model (Self-MoA) could already improve the capability of LLM itself, resulting in new SOTAs on the popular AlpacaEval leaderboard! 

<img width="811" alt="iShot_2024-09-25_20 23 47" src="https://github.com/user-attachments/assets/ff30752e-d48d-4c7c-9d12-13f1900bbae8">

Stay tuned for more details!

[1] Wang, Junlin, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. “Mixture-of-Agents Enhances Large Language Model Capabilities.” arXiv, June 7, 2024. http://arxiv.org/abs/2406.04692.
